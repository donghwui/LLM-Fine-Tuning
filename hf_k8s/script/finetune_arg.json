{
    "model_name_or_path": "meta-llama/Llama-2-7b-hf",
    "dataset_name": "medalpaca/medical_meadow_medical_flashcards",
    "dataset_cache_directory": "/tmp/pvc-mount/hf_dataset_cache",
    "train_file": "",
    "dataset_concatenation": true,
    "prompt_with_input": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.",
    "prompt_without_input": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
    "per_device_train_batch_size": 8,
    "per_device_eval_batch_size": 8,
    "gradient_accumulation_steps": 1,
    "learning_rate": 2e-05,
    "num_train_epochs": 3,
    "max_steps": -1,
    "logging_steps": 10,
    "save_total_limit": 2,
    "output_dir": "/tmp/pvc-mount/output/saved_model",
    "validation_split_percentage": 0.2,
    "log_level": "info",
    "save_strategy": "epoch",
    "ddp_find_unused_parameters": false,
    "ddp_backend": "ccl",
    "use_fast_tokenizer": false,
    "use_lora": true,
    "lora_rank": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "lora_target_modules": [
        "q_proj",
        "v_proj"
    ],
    "no_cuda": true,
    "overwrite_output_dir": true,
    "do_train": true,
    "do_eval": true,
    "bf16": true,
    "use_ipex": true,
    "do_benchmark": false,
    "benchmark_warmup": 30,
    "benchmark_iterations": 300,
    "benchmark_cores_per_instance": -1,
    "benchmark_num_instances": 1,
    "do_quantize": false,
    "peft_model_dir": "/tmp/pvc-mount/output/saved_model",
    "quantize_output_dir": "/tmp/pvc-mount/output/quantized_model",
    "woq_bits": 8,
    "woq_group_size": -1,
    "woq_scheme": "sym",
    "woq_algo": "RTN"
}